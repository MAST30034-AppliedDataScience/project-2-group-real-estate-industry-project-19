{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/10/01 09:45:57 WARN Utils: Your hostname, Honor resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/10/01 09:45:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/01 09:45:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 42906)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/david/project-2-group-real-estate-industry-project-19/project-2-group-real-estate-industry-project-19/.venv/lib/python3.11/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/david/project-2-group-real-estate-industry-project-19/project-2-group-real-estate-industry-project-19/.venv/lib/python3.11/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/home/david/project-2-group-real-estate-industry-project-19/project-2-group-real-estate-industry-project-19/.venv/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/david/project-2-group-real-estate-industry-project-19/project-2-group-real-estate-industry-project-19/.venv/lib/python3.11/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"project2 preprocessing\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "file_path=f\"../data/landing/income.csv\"\n",
    "sa2_code_path='../data/raw/sa2_code_name.csv'\n",
    "output_path=f\"../data/raw/test.csv\"\n",
    "\n",
    "df=spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "df_code=spark.read.csv(sa2_code_path, header=True, inferSchema=True)\n",
    "\n",
    "df = df.withColumnRenamed('sa2_maincode_2016', 'SA2 code')\n",
    "\n",
    "# Columns that should be included in every dataset\n",
    "common_columns = ['FID', 'sa2_maincode_2016', 'geometry', 'sa2_name_2016', 'yr']\n",
    "other_columns = \"estmts_prsnl_incme_yr_endd_30_jne_emplye_ernrs_nm|estmts_prsnl_incme_yr_endd_30_jne_emplye_ernrs_mdn_age_yrs|estimates_personal_income_year_ended_30_june_total_employee_m|estimates_personal_income_year_ended_30_june_median_employee|estimates_personal_income_year_ended_30_june_mean_employee|estmts_prsnl_incme_yr_endd_30_jne_emplye_mn_srce_of_pc|estmts_prsnl_incme_yr_endd_30_jne_unncrprtd_bsnss_ernrs_nm|estm_prsnl_incme_yr_end_30_jne_uncrptd_bsn_erns_mdn_age_yrs|estmts_prsnl_incme_yr_endd_30_jne_ttl_unncrprtd_bsnss_m|estmts_prsnl_incme_yr_endd_30_jne_mdn_unncrprtd_bsnss|estmts_prsnl_incme_yr_endd_30_jne_mn_unncrprtd_bsnss|estmts_prsnl_incme_yr_endd_30_jne_unncrprtd_bsnss_mn_srce_of_pc|estmts_prsnl_incme_yr_endd_30_jne_invstmnt_ernrs_nm|estmts_prsnl_incme_yr_endd_30_jne_invstmnt_ernrs_mdn_age_yrs|estimates_personal_income_year_ended_30_june_total_investment_m|estimates_personal_income_year_ended_30_june_median_investment|estimates_personal_income_year_ended_30_june_mean_investment|estmts_prsnl_incme_yr_endd_30_jne_invstmnt_mn_srce_of_pc|estmts_prsnl_incme_yr_endd_30_jne_sprnntn_annty_ernrs_nm|estm_prsnl_incme_yr_end_30_jne_sprnt_anty_erns_mdn_age_yrs|estmts_prsnl_incme_yr_endd_30_jne_ttl_sprnntn_annty_m|estmts_prsnl_incme_yr_endd_30_jne_mdn_sprnntn_annty|estmts_prsnl_incme_yr_endd_30_jne_mn_sprnntn_annty|estmts_prsnl_incme_yr_endd_30_jne_sprnntn_annty_mn_srce_of_pc|estm_prsnl_incme_yr_end_30_jne_tl_erns_excl_gvrnmt_pns_alwncs_n|estm_prsnl_incme_yr_end_30_jne_tl_erns_excl_gvrnmt_pns_alwncs_m|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_m|estm_prsnl_incme_yr_end_30_jne_mdn_tl_excl_gvrnmt_pns_alwncs|estm_prsnl_incme_yr_end_30_jne_mn_tl_excl_gvrnmt_pns_alwncs|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_p802_r|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_p805_r|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_p205_r|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_p105_r|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_gni_cf|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_shre_o|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_shre_0|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_shre_1|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_lwst_q|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_scnd_q|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_thrd_q|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_hgst_q|grss_cptl_gns_rprtd_txpyrs_yr_endd_30_jne_by_prsns_nm|grss_cptl_gns_rprtd_txpyrs_yr_endd_30_jne_by_amnt_m|grss_cptl_gns_rprtd_txpyrs_yr_endd_30_jne_by_mdn|grss_cptl_gns_rprtd_txpyrs_yr_endd_30_jne_by_mn|slctd_gvrnmnt_pnsns_allwncs_30_jne_age_pnsn_cntrlnk_nm|slctd_gvrnmnt_pnsns_allwncs_30_jne_crr_pymnt_nm|slctd_gvrnmnt_pnsns_allwncs_30_jne_crr_allwnce_nm|slctd_gvrnmnt_pnsns_allwncs_30_jne_dsblty_spprt_pnsn_nm|slctd_gvrnmnt_pnsns_allwncs_30_jne_nwstrt_allwnce_nm|slctd_gvrnmnt_pnsns_allwncs_30_jne_prntng_pymnt_sngle_nm|slctd_gvrnmnt_pnsns_allwncs_30_jne_prntng_pymnt_prtnrd_nm|slctd_gvrnmt_pns_alwncs_30_jne_yth_alwnce_fl_tme_stdnprc_nm|slctd_gvrnmnt_pnsns_allwncs_30_jne_yth_allwnce_nm|slctd_gvrnmnt_pnsns_allwncs_30_jne_fmly_tx_bnft_nm|slctd_gvrnmnt_pnsns_allwncs_30_jne_fmly_tx_bnft_b_nm|slctd_gvrnmnt_pnsns_allwncs_30_jne_cmmnwlth_rnt_assstnce_nm|exprmntl_hshld_wlth_estmts_mdlld_prvte_dwllngs_mn_nt_wrth|ttl_prsnl_incme_wkly_prsns_agd_15_yrs_cnss_ernng_1_499_pr_wk_pc|tl_prsnl_incme_wkly_prsn_agd_15_yrs_cns_erng_500_999_pr_wk_pc|tl_prsnl_incme_wkly_prsn_agd_15_yrs_cns_erng_1000_1999_pr_wk_pc|tl_prsnl_incme_wkly_prsn_agd_15_yrs_cns_erng_2000_2999_pr_wk_pc|ttl_prsnl_incme_wkly_prsns_agd_15_yrs_cnss_ernng_3000_pr_wk_pc|ttl_prsnl_incme_wkly_prsns_agd_15_yrs_cnss_ernng_nl_pc|ttl_prsnl_incme_wkly_prsns_agd_15_yrs_cnss_ngtve_pc|tl_prsnl_incme_wkly_prsn_agd_15_yrs_cns_indqtly_dscrb_std_pc|equivalised_total_household_income_census_median_weekly\".split(\"|\")\n",
    "\n",
    "df=df.join(df_code.select('SA2 code'), on='SA2 code', how='inner')\n",
    "\n",
    "\n",
    "df_cleaned=df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "columns_with_null=[]\n",
    "\n",
    "for column in df_cleaned.columns:\n",
    "    # Filter rows where a column has null values\n",
    "    df_null = df_cleaned.filter(df_cleaned[column].isNull())\n",
    "\n",
    "# Count rows with null values\n",
    "    null_count = df_null.count()\n",
    "\n",
    "    if (null_count>0):\n",
    "        columns_with_null.append(column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, avg\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import coalesce, lag, lead\n",
    "\n",
    "\n",
    "def null_filler(df, column_name):\n",
    "# Create a window specification partitioned by sa2_maincode_2016 and ordered by yr\n",
    "    window_spec = Window.partitionBy('SA2 code').orderBy('yr')\n",
    "\n",
    "# Use lag (previous year) and lead (next year) functions\n",
    "    df_with_adjacent_values = df.withColumn('prev_year_value', lag(column_name).over(window_spec)) \\\n",
    "                            .withColumn('next_year_value', lead(column_name).over(window_spec))\n",
    "\n",
    "\n",
    "# Calculate the average of prev_year_value and next_year_value, ignoring nulls\n",
    "    df_filled = df_with_adjacent_values.withColumn('filled_value',\n",
    "        when(\n",
    "            col(column_name).isNull(), \n",
    "            (coalesce(col('prev_year_value'), col('next_year_value')) + coalesce(col('next_year_value'), col('prev_year_value'))) / 2\n",
    "        ).otherwise(col(column_name))\n",
    "    )\n",
    "\n",
    "# Handle cases where both prev_year_value and next_year_value are null\n",
    "    df_filled = df_filled.withColumn('filled_value',\n",
    "        when(\n",
    "            col('filled_value').isNull(), \n",
    "            lag('prev_year_value', 2).over(window_spec)  # or lead for next 2 years\n",
    "        ).otherwise(col('filled_value'))\n",
    "    )\n",
    "\n",
    "    df_final = df_filled.withColumn(column_name, col('filled_value')).drop('prev_year_value', 'next_year_value', 'filled_value')\n",
    "    return(df_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_filler_multi(df, columns_list):\n",
    "    for column in columns_list:\n",
    "        df=null_filler(df, column)\n",
    "    return df\n",
    "\n",
    "df_filled=null_filler_multi(df_cleaned, columns_with_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/01 09:51:05 WARN BaseSessionStateBuilder$$anon$2: Max iterations (100) reached for batch Operator Optimization before Inferring Filters, please set 'spark.sql.optimizer.maxIterations' to a larger value.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "columns_wanted=['SA2 code', 'sa2_name_2016', 'yr', 'equivalised_total_household_income_census_median_weekly']\n",
    "output_path='../data/raw/income_filled.csv'\n",
    "\n",
    "df=df_filled.select(columns_wanted)\n",
    "df.write.csv(output_path, header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/10/01 11:41:48 WARN Utils: Your hostname, Honor resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/10/01 11:41:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/01 11:41:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/10/01 11:41:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/10/01 11:41:49 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"project2 preprocessing\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "file_path=f\"../data/raw/income_filled.csv\"\n",
    "\n",
    "df=spark.read.csv(file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+---+-------------------------------------------------------+\n",
      "|SA2 code|sa2_name_2016| yr|equivalised_total_household_income_census_median_weekly|\n",
      "+--------+-------------+---+-------------------------------------------------------+\n",
      "+--------+-------------+---+-------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.filter(F.col('SA2 code')==203021487).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"SA2 code\").orderBy(F.desc(\"yr\"))\n",
    "\n",
    "# Step 2: Use the Window function to get the last year value\n",
    "df_last_year_value = (\n",
    "    df.withColumn(\"last_year\", F.max(\"yr\").over(window_spec))  # Get the last year per SA2 code\n",
    "    .filter(F.col(\"yr\") == F.col(\"last_year\"))  # Keep only rows where the year is the last year\n",
    ")\n",
    "\n",
    "# Step 3: Select the desired columns\n",
    "df_last_year_value = df_last_year_value.select(\n",
    "    \"SA2 code\", \n",
    "    \"sa2_name_2016\", \n",
    "    \"equivalised_total_household_income_census_median_weekly\"\n",
    ")\n",
    "\n",
    "\n",
    "output_path=f\"../data/raw/income_only.csv\"\n",
    "\n",
    "df_last_year_value.write.csv(output_path, header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>SA2 code</th><th>sa2_name_2016</th><th>yr</th><th>equivalised_total_household_income_census_median_weekly</th></tr>\n",
       "<tr><td>205031092</td><td>Wilsons Promontory</td><td>2016</td><td>1375.0</td></tr>\n",
       "<tr><td>205031092</td><td>Wilsons Promontory</td><td>2017</td><td>1375.0</td></tr>\n",
       "<tr><td>205051099</td><td>Alps - West</td><td>2011</td><td>350.0</td></tr>\n",
       "<tr><td>205051099</td><td>Alps - West</td><td>2014</td><td>350.0</td></tr>\n",
       "<tr><td>205051099</td><td>Alps - West</td><td>2015</td><td>575.0</td></tr>\n",
       "<tr><td>205051099</td><td>Alps - West</td><td>2016</td><td>575.0</td></tr>\n",
       "<tr><td>205051099</td><td>Alps - West</td><td>2017</td><td>575.0</td></tr>\n",
       "<tr><td>206041120</td><td>Flemington Raceco...</td><td>2011</td><td>1218.0</td></tr>\n",
       "<tr><td>206041120</td><td>Flemington Raceco...</td><td>2014</td><td>1218.0</td></tr>\n",
       "<tr><td>206041120</td><td>Flemington Raceco...</td><td>2015</td><td>1512.0</td></tr>\n",
       "<tr><td>206041120</td><td>Flemington Raceco...</td><td>2016</td><td>1512.0</td></tr>\n",
       "<tr><td>206041120</td><td>Flemington Raceco...</td><td>2017</td><td>1512.0</td></tr>\n",
       "<tr><td>206041127</td><td>West Melbourne</td><td>2017</td><td>NULL</td></tr>\n",
       "<tr><td>208031192</td><td>Moorabbin Airport</td><td>2017</td><td>NULL</td></tr>\n",
       "<tr><td>210011227</td><td>Essendon Airport</td><td>2016</td><td>NULL</td></tr>\n",
       "<tr><td>210011227</td><td>Essendon Airport</td><td>2017</td><td>NULL</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+--------------------+----+-------------------------------------------------------+\n",
       "| SA2 code|       sa2_name_2016|  yr|equivalised_total_household_income_census_median_weekly|\n",
       "+---------+--------------------+----+-------------------------------------------------------+\n",
       "|205031092|  Wilsons Promontory|2016|                                                 1375.0|\n",
       "|205031092|  Wilsons Promontory|2017|                                                 1375.0|\n",
       "|205051099|         Alps - West|2011|                                                  350.0|\n",
       "|205051099|         Alps - West|2014|                                                  350.0|\n",
       "|205051099|         Alps - West|2015|                                                  575.0|\n",
       "|205051099|         Alps - West|2016|                                                  575.0|\n",
       "|205051099|         Alps - West|2017|                                                  575.0|\n",
       "|206041120|Flemington Raceco...|2011|                                                 1218.0|\n",
       "|206041120|Flemington Raceco...|2014|                                                 1218.0|\n",
       "|206041120|Flemington Raceco...|2015|                                                 1512.0|\n",
       "|206041120|Flemington Raceco...|2016|                                                 1512.0|\n",
       "|206041120|Flemington Raceco...|2017|                                                 1512.0|\n",
       "|206041127|      West Melbourne|2017|                                                   NULL|\n",
       "|208031192|   Moorabbin Airport|2017|                                                   NULL|\n",
       "|210011227|    Essendon Airport|2016|                                                   NULL|\n",
       "|210011227|    Essendon Airport|2017|                                                   NULL|\n",
       "+---------+--------------------+----+-------------------------------------------------------+"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idlist=[208031192,205051099,206041120,205031092,206041127,210011227, 205021080]\n",
    "\n",
    "df.filter(col('SA2 code').isin(idlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "| SA2 code|has_2019|\n",
      "+---------+--------+\n",
      "|208031192|       0|\n",
      "|205021080|       0|\n",
      "|205051099|       0|\n",
      "|206041120|       0|\n",
      "|205031092|       0|\n",
      "|206041127|       0|\n",
      "|210011227|       0|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming the original DataFrame 'df' is already created\n",
    "\n",
    "# Group by 'id' and aggregate to check if there's any row with year 2019\n",
    "df_no_2019 = df.groupBy(\"SA2 code\").agg(\n",
    "    F.max(F.when(col(\"yr\") == 2019, 1).otherwise(0)).alias(\"has_2019\")\n",
    ")\n",
    "\n",
    "# Filter for IDs that do not have year 2019\n",
    "df_no_2019_filtered = df_no_2019.filter(col(\"has_2019\") == 0)\n",
    "\n",
    "# Show the result\n",
    "df_no_2019_filtered.show()\n",
    "\n",
    "df_joined = df_no_2019_filtered.join(df, on='SA2 code')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+----+-------------------------------------------------------+\n",
      "| SA2 code|    sa2_name_2016|  yr|equivalised_total_household_income_census_median_weekly|\n",
      "+---------+-----------------+----+-------------------------------------------------------+\n",
      "|210051248|Melbourne Airport|2019|                                                   NULL|\n",
      "+---------+-----------------+----+-------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2019_nulls = df.filter(col('yr') == 2019)\n",
    "df_2019_nulls=df_2019_nulls.filter(col('equivalised_total_household_income_census_median_weekly').isNull())\n",
    "df_2019_nulls.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+--------------------+--------------------+\n",
      "| SA2 code|value_2011|value_2019|                CAGR|       sa2_name_2016|\n",
      "+---------+----------+----------+--------------------+--------------------+\n",
      "|201011001|     786.0|     914.0|0.019038180747419586|           Alfredton|\n",
      "|201011002|     762.0|     903.0| 0.02144878781001558|            Ballarat|\n",
      "|201011005|     812.0|     920.0|0.015731625603674804|           Buninyong|\n",
      "|201011006|     643.0|     761.0|0.021284428971496627|           Delacombe|\n",
      "|201011007|     736.0|     867.0|0.020687180934804372|       Smythes Creek|\n",
      "|201011008|     565.0|     635.0|0.014707007710210496|Wendouree - Miner...|\n",
      "|201021009|     697.0|     837.0| 0.02314358351846635|Bacchus Marsh Region|\n",
      "|201021010|     568.0|     643.0|0.015623706758379097|   Creswick - Clunes|\n",
      "|201021011|     576.0|     689.0|0.022644277110845712|          Daylesford|\n",
      "|201021012|     691.0|     782.0|0.015584556648718495|       Gordon (Vic.)|\n",
      "|201031013|     459.0|     535.0| 0.01933664440418581|               Avoca|\n",
      "|201031014|     531.0|     607.0|0.016861421998789927|            Beaufort|\n",
      "|201031015|     594.0|     687.0|0.018348168167021672|Golden Plains - N...|\n",
      "|201031016|     480.0|     525.0| 0.01126449176699662|  Maryborough (Vic.)|\n",
      "|201031017|     476.0|     509.0| 0.00841397042828218|  Maryborough Region|\n",
      "|202011018|     594.0|     705.0| 0.02164575306893357|             Bendigo|\n",
      "|202011019|     546.0|     629.0|0.017846412684100477|California Gully ...|\n",
      "|202011020|     706.0|     776.0| 0.01188725882501851|East Bendigo - Ke...|\n",
      "|202011021|     682.0|     770.0|0.015285757268941103|Flora Hill - Spri...|\n",
      "|202011022|     609.0|     697.0|0.017014010100258936|Kangaroo Flat - G...|\n",
      "+---------+----------+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2011 = df.filter(col('yr') == 2011).select('SA2 code', col('equivalised_total_household_income_census_median_weekly').alias('value_2011'))\n",
    "df_2019 = df.filter(col('yr') == 2019).select('SA2 code', col('equivalised_total_household_income_census_median_weekly').alias('value_2019'))\n",
    "\n",
    "# Join the two DataFrames on 'id'\n",
    "df_joined = df_2011.join(df_2019, on='SA2 code')\n",
    "\n",
    "# Calculate the CAGR\n",
    "df_with_cagr = df_joined.withColumn(\n",
    "    'CAGR', \n",
    "    (pow(col('value_2019') / col('value_2011'), 1 / 8) - 1)\n",
    ")\n",
    "\n",
    "df_with_cagr=df_with_cagr.join(df.select('SA2 code', 'sa2_name_2016').dropDuplicates([\"SA2 code\"]), on='SA2 code')\n",
    "\n",
    "# Show the result\n",
    "df_with_cagr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"project2 preprocessing\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "df_wanted=df_with_cagr.withColumnRenamed('CAGR', 'avg_yearly_growth_rate')\n",
    "columns_wanted=['SA2 code', 'sa2_name_2016', 'value_2019', \"avg_yearly_growth_rate\"]\n",
    "\n",
    "output_path=f\"../data/raw/income.csv\"\n",
    "df_wanted=df_wanted.select(columns_wanted)\n",
    "df_wanted.write.csv(output_path, header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs missing from DataFrame: 522\n"
     ]
    }
   ],
   "source": [
    "df_full=spark.read.csv('../data/raw/population.csv', header=True)\n",
    "\n",
    "# Extract distinct ids from the DataFrame\n",
    "distinct_ids_df = df_full.withColumnRenamed(\"SA2 code\", 'id').select('id').distinct()\n",
    "\n",
    "# Collect the distinct ids into a Python list\n",
    "existing_ids = distinct_ids_df.rdd.map(lambda row: row.id).collect()\n",
    "\n",
    "# Convert the existing ids to a set\n",
    "id_set = set(existing_ids)\n",
    "\n",
    "# Extract distinct ids from the DataFrame\n",
    "distinct_ids_df = df_wanted.withColumnRenamed(\"SA2 code\", 'id').select(\"id\").distinct()\n",
    "\n",
    "# Collect the distinct ids into a Python list\n",
    "existing_ids = distinct_ids_df.rdd.map(lambda row: row.id).collect()\n",
    "\n",
    "# Convert the existing ids to a set\n",
    "existing_ids_set = set(existing_ids)\n",
    "\n",
    "# Find the ids that are in id_list but not in the DataFrame\n",
    "missing_ids = id_set - existing_ids_set\n",
    "\n",
    "# Show the missing ids\n",
    "print(f\"IDs missing from DataFrame: {len(missing_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+----------+--------------------+\n",
      "|      _c0|                 _c1|       _c2|                 _c3|\n",
      "+---------+--------------------+----------+--------------------+\n",
      "| SA2 code|       sa2_name_2016|value_2019|avg_yearly_growth...|\n",
      "|201011001|           Alfredton|     914.0|0.019038180747419586|\n",
      "|201011002|            Ballarat|     903.0| 0.02144878781001558|\n",
      "|201011005|           Buninyong|     920.0|0.015731625603674804|\n",
      "|201011006|           Delacombe|     761.0|0.021284428971496627|\n",
      "|201011007|       Smythes Creek|     867.0|0.020687180934804372|\n",
      "|201011008|Wendouree - Miner...|     635.0|0.014707007710210496|\n",
      "|201021009|Bacchus Marsh Region|     837.0| 0.02314358351846635|\n",
      "|201021010|   Creswick - Clunes|     643.0|0.015623706758379097|\n",
      "|201021011|          Daylesford|     689.0|0.022644277110845712|\n",
      "|201021012|       Gordon (Vic.)|     782.0|0.015584556648718495|\n",
      "|201031013|               Avoca|     535.0| 0.01933664440418581|\n",
      "|201031014|            Beaufort|     607.0|0.016861421998789927|\n",
      "|201031015|Golden Plains - N...|     687.0|0.018348168167021672|\n",
      "|201031016|  Maryborough (Vic.)|     525.0| 0.01126449176699662|\n",
      "|201031017|  Maryborough Region|     509.0| 0.00841397042828218|\n",
      "|202011018|             Bendigo|     705.0| 0.02164575306893357|\n",
      "|202011019|California Gully ...|     629.0|0.017846412684100477|\n",
      "|202011020|East Bendigo - Ke...|     776.0| 0.01188725882501851|\n",
      "|202011021|Flora Hill - Spri...|     770.0|0.015285757268941103|\n",
      "+---------+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"project2 preprocessing\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "df=spark.read.csv(output_path)\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
