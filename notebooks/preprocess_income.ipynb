{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read income dataset and filter the part from victoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/10/18 09:24:54 WARN Utils: Your hostname, Honor resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/10/18 09:24:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/18 09:24:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read income dataset and filter the part from victoria\n",
    "# dataset is downloaded from https://adp-access.aurin.org.au/dataset/au-govt-abs-abs-data-by-region-income-asgs-sa2-2011-2019-sa2-2016\n",
    "# dataset download is renamed as income.csv and placed in /data/landing dictionary\n",
    "# the website requires licence to download dataset so it cannot be done through code scraping\n",
    "\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"project2 preprocessing\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "file_path=f\"../data/landing/income.csv\"\n",
    "sa2_code_path='../data/raw/sa2_code_name.csv'\n",
    "output_path=f\"../data/raw/test.csv\"\n",
    "\n",
    "df=spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "df_code=spark.read.csv(sa2_code_path, header=True, inferSchema=True)\n",
    "\n",
    "df = df.withColumnRenamed('sa2_maincode_2016', 'SA2 code')\n",
    "\n",
    "# Columns that should be included in every dataset\n",
    "common_columns = ['FID', 'sa2_maincode_2016', 'geometry', 'sa2_name_2016', 'yr']\n",
    "other_columns = \"estmts_prsnl_incme_yr_endd_30_jne_emplye_ernrs_nm|estmts_prsnl_incme_yr_endd_30_jne_emplye_ernrs_mdn_age_yrs|estimates_personal_income_year_ended_30_june_total_employee_m|estimates_personal_income_year_ended_30_june_median_employee|estimates_personal_income_year_ended_30_june_mean_employee|estmts_prsnl_incme_yr_endd_30_jne_emplye_mn_srce_of_pc|estmts_prsnl_incme_yr_endd_30_jne_unncrprtd_bsnss_ernrs_nm|estm_prsnl_incme_yr_end_30_jne_uncrptd_bsn_erns_mdn_age_yrs|estmts_prsnl_incme_yr_endd_30_jne_ttl_unncrprtd_bsnss_m|estmts_prsnl_incme_yr_endd_30_jne_mdn_unncrprtd_bsnss|estmts_prsnl_incme_yr_endd_30_jne_mn_unncrprtd_bsnss|estmts_prsnl_incme_yr_endd_30_jne_unncrprtd_bsnss_mn_srce_of_pc|estmts_prsnl_incme_yr_endd_30_jne_invstmnt_ernrs_nm|estmts_prsnl_incme_yr_endd_30_jne_invstmnt_ernrs_mdn_age_yrs|estimates_personal_income_year_ended_30_june_total_investment_m|estimates_personal_income_year_ended_30_june_median_investment|estimates_personal_income_year_ended_30_june_mean_investment|estmts_prsnl_incme_yr_endd_30_jne_invstmnt_mn_srce_of_pc|estmts_prsnl_incme_yr_endd_30_jne_sprnntn_annty_ernrs_nm|estm_prsnl_incme_yr_end_30_jne_sprnt_anty_erns_mdn_age_yrs|estmts_prsnl_incme_yr_endd_30_jne_ttl_sprnntn_annty_m|estmts_prsnl_incme_yr_endd_30_jne_mdn_sprnntn_annty|estmts_prsnl_incme_yr_endd_30_jne_mn_sprnntn_annty|estmts_prsnl_incme_yr_endd_30_jne_sprnntn_annty_mn_srce_of_pc|estm_prsnl_incme_yr_end_30_jne_tl_erns_excl_gvrnmt_pns_alwncs_n|estm_prsnl_incme_yr_end_30_jne_tl_erns_excl_gvrnmt_pns_alwncs_m|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_m|estm_prsnl_incme_yr_end_30_jne_mdn_tl_excl_gvrnmt_pns_alwncs|estm_prsnl_incme_yr_end_30_jne_mn_tl_excl_gvrnmt_pns_alwncs|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_p802_r|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_p805_r|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_p205_r|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_p105_r|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_gni_cf|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_shre_o|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_shre_0|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_shre_1|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_lwst_q|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_scnd_q|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_thrd_q|estm_prsnl_incme_yr_end_30_jne_tl_excl_gvrnmt_pns_alwncs_hgst_q|grss_cptl_gns_rprtd_txpyrs_yr_endd_30_jne_by_prsns_nm|grss_cptl_gns_rprtd_txpyrs_yr_endd_30_jne_by_amnt_m|grss_cptl_gns_rprtd_txpyrs_yr_endd_30_jne_by_mdn|grss_cptl_gns_rprtd_txpyrs_yr_endd_30_jne_by_mn|slctd_gvrnmnt_pnsns_allwncs_30_jne_age_pnsn_cntrlnk_nm|slctd_gvrnmnt_pnsns_allwncs_30_jne_crr_pymnt_nm|slctd_gvrnmnt_pnsns_allwncs_30_jne_crr_allwnce_nm|slctd_gvrnmnt_pnsns_allwncs_30_jne_dsblty_spprt_pnsn_nm|slctd_gvrnmnt_pnsns_allwncs_30_jne_nwstrt_allwnce_nm|slctd_gvrnmnt_pnsns_allwncs_30_jne_prntng_pymnt_sngle_nm|slctd_gvrnmnt_pnsns_allwncs_30_jne_prntng_pymnt_prtnrd_nm|slctd_gvrnmt_pns_alwncs_30_jne_yth_alwnce_fl_tme_stdnprc_nm|slctd_gvrnmnt_pnsns_allwncs_30_jne_yth_allwnce_nm|slctd_gvrnmnt_pnsns_allwncs_30_jne_fmly_tx_bnft_nm|slctd_gvrnmnt_pnsns_allwncs_30_jne_fmly_tx_bnft_b_nm|slctd_gvrnmnt_pnsns_allwncs_30_jne_cmmnwlth_rnt_assstnce_nm|exprmntl_hshld_wlth_estmts_mdlld_prvte_dwllngs_mn_nt_wrth|ttl_prsnl_incme_wkly_prsns_agd_15_yrs_cnss_ernng_1_499_pr_wk_pc|tl_prsnl_incme_wkly_prsn_agd_15_yrs_cns_erng_500_999_pr_wk_pc|tl_prsnl_incme_wkly_prsn_agd_15_yrs_cns_erng_1000_1999_pr_wk_pc|tl_prsnl_incme_wkly_prsn_agd_15_yrs_cns_erng_2000_2999_pr_wk_pc|ttl_prsnl_incme_wkly_prsns_agd_15_yrs_cnss_ernng_3000_pr_wk_pc|ttl_prsnl_incme_wkly_prsns_agd_15_yrs_cnss_ernng_nl_pc|ttl_prsnl_incme_wkly_prsns_agd_15_yrs_cnss_ngtve_pc|tl_prsnl_incme_wkly_prsn_agd_15_yrs_cns_indqtly_dscrb_std_pc|equivalised_total_household_income_census_median_weekly\".split(\"|\")\n",
    "\n",
    "df=df.join(df_code.select('SA2 code'), on='SA2 code', how='inner')\n",
    "\n",
    "\n",
    "df_cleaned=df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find out all the columns with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "columns_with_null=[]\n",
    "\n",
    "for column in df_cleaned.columns:\n",
    "    df_null = df_cleaned.filter(df_cleaned[column].isNull())\n",
    "\n",
    "    null_count = df_null.count()\n",
    "\n",
    "    if (null_count>0):\n",
    "        columns_with_null.append(column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a function filling null values based on value of the same area from previous and following year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, avg\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import coalesce, lag, lead\n",
    "\n",
    "\n",
    "def null_filler(df, column_name):\n",
    "# Create a window specification partitioned by sa2_maincode_2016 and ordered by yr\n",
    "    window_spec = Window.partitionBy('SA2 code').orderBy('yr')\n",
    "\n",
    "# Use lag (previous year) and lead (next year) functions\n",
    "    df_with_adjacent_values = df.withColumn('prev_year_value', lag(column_name).over(window_spec)) \\\n",
    "                            .withColumn('next_year_value', lead(column_name).over(window_spec))\n",
    "\n",
    "\n",
    "# Calculate the average of prev_year_value and next_year_value, ignoring nulls\n",
    "    df_filled = df_with_adjacent_values.withColumn('filled_value',\n",
    "        when(\n",
    "            col(column_name).isNull(), \n",
    "            (coalesce(col('prev_year_value'), col('next_year_value')) + coalesce(col('next_year_value'), col('prev_year_value'))) / 2\n",
    "        ).otherwise(col(column_name))\n",
    "    )\n",
    "\n",
    "# Handle cases where both prev_year_value and next_year_value are null\n",
    "    df_filled = df_filled.withColumn('filled_value',\n",
    "        when(\n",
    "            col('filled_value').isNull(), \n",
    "            lag('prev_year_value', 2).over(window_spec) \n",
    "        ).otherwise(col('filled_value'))\n",
    "    )\n",
    "\n",
    "    df_final = df_filled.withColumn(column_name, col('filled_value')).drop('prev_year_value', 'next_year_value', 'filled_value')\n",
    "    return(df_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a function that accept a list of column names and fill the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_filler_multi(df, columns_list):\n",
    "    for column in columns_list:\n",
    "        df=null_filler(df, column)\n",
    "    return df\n",
    "\n",
    "df_filled=null_filler_multi(df_cleaned, columns_with_null)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter the necessary columns and save them for further study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/18 09:26:39 WARN BaseSessionStateBuilder$$anon$2: Max iterations (100) reached for batch Operator Optimization before Inferring Filters, please set 'spark.sql.optimizer.maxIterations' to a larger value.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "columns_wanted=['SA2 code', 'sa2_name_2016', 'yr', 'equivalised_total_household_income_census_median_weekly']\n",
    "output_path='../data/raw/income_filled.csv'\n",
    "\n",
    "df=df_filled.select(columns_wanted)\n",
    "df.write.csv(output_path, header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read the filled income dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"project2 preprocessing\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "file_path=f\"../data/raw/income_filled.csv\"\n",
    "\n",
    "df=spark.read.csv(file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select the values of income for each suburbs in the latest year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"SA2 code\").orderBy(F.desc(\"yr\"))\n",
    "\n",
    "\n",
    "df_last_year_value = (\n",
    "    df.withColumn(\"last_year\", F.max(\"yr\").over(window_spec))  \n",
    "    .filter(F.col(\"yr\") == F.col(\"last_year\")) \n",
    ")\n",
    "\n",
    "df_last_year_value = df_last_year_value.select(\n",
    "    \"SA2 code\", \n",
    "    \"sa2_name_2016\", \n",
    "    \"equivalised_total_household_income_census_median_weekly\"\n",
    ")\n",
    "\n",
    "# keep a record of the latest income data\n",
    "output_path=f\"../data/raw/income_only.csv\"\n",
    "\n",
    "df_last_year_value.write.csv(output_path, header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find average yearly growth rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+--------------------+--------------------+\n",
      "| SA2 code|value_2011|value_2019|                CAGR|       sa2_name_2016|\n",
      "+---------+----------+----------+--------------------+--------------------+\n",
      "|201011001|     786.0|     914.0|0.019038180747419586|           Alfredton|\n",
      "|201011002|     762.0|     903.0| 0.02144878781001558|            Ballarat|\n",
      "|201011005|     812.0|     920.0|0.015731625603674804|           Buninyong|\n",
      "|201011006|     643.0|     761.0|0.021284428971496627|           Delacombe|\n",
      "|201011007|     736.0|     867.0|0.020687180934804372|       Smythes Creek|\n",
      "|201011008|     565.0|     635.0|0.014707007710210496|Wendouree - Miner...|\n",
      "|201021009|     697.0|     837.0| 0.02314358351846635|Bacchus Marsh Region|\n",
      "|201021010|     568.0|     643.0|0.015623706758379097|   Creswick - Clunes|\n",
      "|201021011|     576.0|     689.0|0.022644277110845712|          Daylesford|\n",
      "|201021012|     691.0|     782.0|0.015584556648718495|       Gordon (Vic.)|\n",
      "|201031013|     459.0|     535.0| 0.01933664440418581|               Avoca|\n",
      "|201031014|     531.0|     607.0|0.016861421998789927|            Beaufort|\n",
      "|201031015|     594.0|     687.0|0.018348168167021672|Golden Plains - N...|\n",
      "|201031016|     480.0|     525.0| 0.01126449176699662|  Maryborough (Vic.)|\n",
      "|201031017|     476.0|     509.0| 0.00841397042828218|  Maryborough Region|\n",
      "|202011018|     594.0|     705.0| 0.02164575306893357|             Bendigo|\n",
      "|202011019|     546.0|     629.0|0.017846412684100477|California Gully ...|\n",
      "|202011020|     706.0|     776.0| 0.01188725882501851|East Bendigo - Ke...|\n",
      "|202011021|     682.0|     770.0|0.015285757268941103|Flora Hill - Spri...|\n",
      "|202011022|     609.0|     697.0|0.017014010100258936|Kangaroo Flat - G...|\n",
      "+---------+----------+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add columns with income data from first and last year\n",
    "\n",
    "df_2011 = df.filter(col('yr') == 2011).select('SA2 code', col('equivalised_total_household_income_census_median_weekly').alias('value_2011'))\n",
    "df_2019 = df.filter(col('yr') == 2019).select('SA2 code', col('equivalised_total_household_income_census_median_weekly').alias('value_2019'))\n",
    "\n",
    "df_joined = df_2011.join(df_2019, on='SA2 code')\n",
    "\n",
    "# Calculate the average growth rate\n",
    "df_with_cagr = df_joined.withColumn(\n",
    "    'CAGR', \n",
    "    (pow(col('value_2019') / col('value_2011'), 1 / 8) - 1)\n",
    ")\n",
    "\n",
    "df_with_cagr=df_with_cagr.join(df.select('SA2 code', 'sa2_name_2016').dropDuplicates([\"SA2 code\"]), on='SA2 code')\n",
    "\n",
    "# Show the result\n",
    "df_with_cagr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the dataset for further study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"project2 preprocessing\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "df_wanted=df_with_cagr.withColumnRenamed('CAGR', 'avg_yearly_growth_rate')\n",
    "columns_wanted=['SA2 code', 'sa2_name_2016', 'value_2019', \"avg_yearly_growth_rate\"]\n",
    "\n",
    "output_path=f\"../data/raw/income.csv\"\n",
    "df_wanted=df_wanted.select(columns_wanted)\n",
    "df_wanted.write.csv(output_path, header=True, mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
